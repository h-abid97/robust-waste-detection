<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Robust and Label-Efficient Deep Waste Detection — BMVC 2025</title>
  <meta name="description" content="BMVC 2025 project page: Robust and Label-Efficient Deep Waste Detection — zero-shot OVOD benchmarks, fine-tuned transformer baselines, and ensemble-based semi-supervised learning for industrial waste detection." />
  <meta property="og:title" content="Robust and Label-Efficient Deep Waste Detection — BMVC 2025" />
  <meta property="og:description" content="ZeroWaste OVOD benchmarks, 51.6 mAP fine-tuned baseline, and soft ensemble pseudo-labeling that surpasses fully supervised training." />
  <meta property="og:image" content="https://h-abid97.github.io/robust-waste-detection/assets/teaser.png" />
  <meta name="twitter:card" content="summary_large_image" />
  <link rel="stylesheet" href="css/style.css" />
</head>
<body>
  <main class="wrap">
    <h1>Robust and Label-Efficient Deep Waste Detection</h1>
    <div class="meta">
      Hassan Abid<sup>1</sup>, Khan Muhammad<sup>2</sup>, Muhammad Haris Khan<sup>1</sup><br>
      <em>BMVC 2025</em><br>
      <sup>1</sup> Mohamed Bin Zayed University of Artificial Intelligence (MBZUAI), Abu Dhabi, UAE<br>
      <sup>2</sup> Sungkyunkwan University, Seoul, South Korea
    </div>

    <p class="links">
      <a href="https://arxiv.org/abs/2508.18799" target="_blank">arXiv</a>
      <a href="https://github.com/h-abid97/robust-waste-detection" target="_blank">Code (GitHub)</a>
      <a href="bib/citation.bib">Citation (BibTeX)</a>
    </p>

    <!-- Abstract -->
    <h2>Abstract</h2>
    <p>
      Effective waste sorting is critical for sustainable recycling, yet AI research in this domain lags behind commercial systems due to limited datasets and reliance on outdated detectors. 
      We advance AI-driven waste detection by <strong>benchmarking open-vocabulary detectors</strong>, <strong>establishing strong supervised baselines</strong>, and <strong>proposing a robust semi-supervised framework</strong>. 
      Using the <em>ZeroWaste</em> dataset, we demonstrate that:
      (1) LLM-optimized prompts improve zero-shot detection, 
      (2) fine-tuned transformer-based detectors set new baselines of <strong>51.6 mAP</strong>, and 
      (3) ensemble-based soft pseudo-labeling produces high-quality annotations for 6,065 unlabeled images, boosting downstream detectors beyond fully supervised training.
    </p>

    <!-- Highlights -->
  <h2>Highlights</h2>
  <div class="highlights-grid">
    <div class="highlight-card">
      <h3>Zero-shot OVOD</h3>
      <p>Class-only prompts underperform, but <strong>LLM-optimized prompts</strong> boost OWLv2 from <strong>7.3 → 13.5 mAP</strong>.</p>
    </div>
    <div class="highlight-card">
      <h3>Fine-tuned Baselines</h3>
      <p>Co-DETR, DETA, and Grounding DINO (Swin-B) achieve <strong>51.6 mAP</strong>, more than <strong>2× stronger</strong> than CNN baselines.</p>
    </div>
    <div class="highlight-card">
      <h3>Semi-Supervised</h3>
      <p>Ensemble-based <em>soft</em> pseudo-labels push Grounding DINO (Swin-B) to <strong>54.3 mAP</strong>.</p>
    </div>
    <div class="highlight-card">
      <h3>Final Dataset</h3>
      <p>Released <strong>33,075 pseudo-annotations</strong> for ZeroWaste-s, improving YOLO11 (+6.3 mAP) and RT-DETR (+4.3 mAP).</p>
    </div>
  </div>

    <!-- Method -->
    <h2>Method Overview</h2>
    <p>
      Our framework combines three pillars:
      (1) zero-shot benchmarking of OVOD models on industrial waste data, 
      (2) supervised fine-tuning of modern detectors, and 
      (3) ensemble-based pseudo-labeling to exploit unlabeled data. 
      The pipeline fuses predictions across models using Weighted Box Fusion and adjusts confidence scores via spatial consistency and inter-model agreement.
    </p>

    <!-- Results -->
    <h2>Results</h2>
    <h3>Zero-Shot OVOD Evaluation</h3>
    <img src="assets/optimized_queries_pipeline.png" alt="Pipeline for generating optimized queries">
    <p>
      Grounding DINO, OWLv2, and YOLO-World show weak performance on ZeroWaste with class-only prompts (≤7.3 mAP). 
      Using GPT-4o optimized prompts substantially boosts performance (OWLv2: +6.2 mAP).
    </p>
    <img src="assets/zs_class_only_bm.png" alt="Zero-shot OVOD results">
    <img src="assets/zs_class_only_v_optimized.png" alt="Zero-shot OVOD results">

    <h3>Fine-Tuned Baselines</h3>
    <p>
      Transformer-based detectors dramatically outperform CNN baselines. 
      Co-DETR (Swin-L), DETA (Swin-L), and Grounding DINO (Swin-B) each achieve <strong>51.6 mAP</strong>, surpassing TridentNet’s 24.2 mAP.
    </p>
    <img src="assets/sft_benchmark.png" alt="Fine-tuned baselines results">

    <h3>Semi-Supervised Learning</h3>
    <img src="assets/optimized_queries_pipeline.png" alt="Pipeline of ensemble-based pseudo-labeling">
    <p>
      Our soft ensemble pseudo-labeling strategy significantly improves detection. 
      Grounding DINO (Swin-B) rises from 51.6 → 54.3 mAP, with consistent per-class gains including rare categories like <em>metal</em>.
    </p>
    <img src="assets/semi_sup_results.png" alt="Semi-supervised learning results">

    <!-- Dataset -->
    <h2>Dataset & Settings</h2>
    <p>
      We adopt the <strong>ZeroWaste</strong> dataset collected from an industrial Material Recovery Facility:
    </p>
    <ul>
      <li><strong>ZeroWaste-f:</strong> 4,503 labeled images with bounding boxes for cardboard, soft plastic, rigid plastic, and metal.</li>
      <li><strong>ZeroWaste-s:</strong> 6,212 unlabeled images for semi-supervised learning.</li>
      <li><strong>Challenges:</strong> high clutter, occlusion, deformation, and severe class imbalance.</li>
    </ul>
    <img src="assets/zerowaste-f.png" alt="Dataset samples">

    <!-- Final Pseudo Annotations -->
    <h2>Final Pseudo-Annotations</h2>
    <p>
      Our ensemble-generated pseudo-labels for ZeroWaste-s yield <strong>33,075 bounding boxes</strong> across 6,065 images. 
      Training detectors solely on these annotations provides consistent gains:
    </p>
    <ul>
      <li>YOLO11 (Large): +6.3 mAP</li>
      <li>RT-DETR (Large): +4.3 mAP</li>
    </ul>
    <img src="assets/final_pseudo_annotations.png" alt="Pseudo-annotations statistics and examples">

    <h2>Citation</h2>
    <pre id="bibtex"><code>@misc{abid2025robustlabelefficientdeepwaste,
  title         = {Robust and Label-Efficient Deep Waste Detection},
  author        = {Hassan Abid and Khan Muhammad and Muhammad Haris Khan},
  year          = {2025},
  eprint        = {2508.18799},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CV},
  url           = {https://arxiv.org/abs/2508.18799}
}</code></pre>

    <div style="margin-top:8px;">
      <button id="copy-btn">Copy BibTeX</button>
    </div>

    <footer>© 2025 Hassan Abid, Khan Muhammad, Muhammad Haris Khan</footer>
  </main>

  <script>
    const copyBtn = document.getElementById("copy-btn");
    copyBtn.addEventListener("click", () => {
      const bibtex = document.getElementById("bibtex").innerText;
      navigator.clipboard.writeText(bibtex).then(() => {
        copyBtn.textContent = "Copied!";
        setTimeout(() => copyBtn.textContent = "Copy BibTeX", 2000);
      }).catch(err => {
        console.error("Copy failed", err);
      });
    });
  </script>
</body>
</html>